{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (528493, 8)\n",
      "Test dataset shape: (96, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_file_path = \"Updated_TrainingData.xlsx\"  \n",
    "test_file_path = \"Updated_TestData.xlsx\" \n",
    "train_data = pd.read_excel(train_file_path)\n",
    "test_data = pd.read_excel(test_file_path)\n",
    "\n",
    "X_train = train_data.drop(columns=['Sourcing Cost'])  \n",
    "y_train = train_data['Sourcing Cost']  \n",
    "\n",
    "X_test = test_data.drop(columns=['Sourcing Cost'])  \n",
    "y_test = test_data['Sourcing Cost']  \n",
    "\n",
    "print(\"Training dataset shape:\", X_train.shape)\n",
    "print(\"Test dataset shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor Model:\n",
    "\n",
    "- **High Accuracy:** RandomForestRegressor typically yields high accuracy in prediction tasks due to its ability to capture complex non-linear relationships in the data, making it suitable for multivariate datasets.\n",
    "\n",
    "- **Robust to Overfitting:** By constructing multiple decision trees and averaging their predictions, RandomForestRegressor is less prone to overfitting compared to individual decision trees, ensuring better generalization performance.\n",
    "\n",
    "- **Feature Importance:** RandomForestRegressor provides a feature importance score, allowing you to understand which features have the most significant impact on predictions, aiding in feature selection and interpretation of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Random Forest RMSE: 90.03733081700386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rf_model_tuned = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=2)\n",
    "\n",
    "rf_model_tuned.fit(X_train, y_train)\n",
    "\n",
    "rf_preds_tuned = rf_model_tuned.predict(X_test)\n",
    "\n",
    "rf_rmse_tuned = mean_squared_error(y_test, rf_preds_tuned, squared=False)\n",
    "print(\"Tuned Random Forest RMSE:\", rf_rmse_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBRegressor Model:\n",
    "\n",
    "- **Excellent Performance:** XGBoost is known for its exceptional performance in both speed and accuracy, making it well-suited for handling complex datasets efficiently.\n",
    "\n",
    "- **Regularization Techniques:** XGBoost includes built-in regularization techniques such as L1 and L2 regularization, which help prevent overfitting and improve generalization performance.\n",
    "\n",
    "- **Feature Importance:** XGBRegressor provides feature importance scores, enabling you to identify the most influential features in your dataset and interpret the model's predictions effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost RMSE: 84.35457812098922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "xgb_model = XGBRegressor()\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "\n",
    "xgb_rmse = mean_squared_error(y_test, xgb_preds, squared=False)\n",
    "print(\"XGBoost RMSE:\", xgb_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBMRegressor Model:\n",
    "\n",
    "1. **Efficient Training Speed**: LGBMRegressor is known for its fast training speed, making it efficient for handling large datasets and reducing computational time. This characteristic is particularly beneficial for training models on your multivariate dataset.\n",
    "\n",
    "2. **High Accuracy**: Despite its speed, LGBMRegressor often achieves high accuracy in prediction tasks, thanks to its gradient boosting framework and advanced tree-based algorithms. This ensures reliable predictions for your dataset.\n",
    "\n",
    "3. **Handling of Categorical Features**: LGBMRegressor naturally handles categorical features without the need for one-hot encoding. This simplifies the preprocessing step and allows for better utilization of the features in your multivariate dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 81\n",
      "[LightGBM] [Info] Number of data points in the train set: 528493, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 110.035407\n",
      "LightGBM RMSE: 83.91287828231486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lgbm_model = LGBMRegressor()\n",
    "\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "lgbm_preds = lgbm_model.predict(X_test)\n",
    "\n",
    "lgbm_rmse = mean_squared_error(y_test, lgbm_preds, squared=False)\n",
    "print(\"LightGBM RMSE:\", lgbm_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR (Vector Auto Regression) Model:\n",
    "\n",
    "1. **Multivariate Forecasting**: VAR model allows for multivariate forecasting, making it suitable for datasets with multiple correlated variables, like your dataset with multiple features.\n",
    "\n",
    "2. **Capturing Dynamic Interactions**: VAR captures the dynamic interactions between variables over time, allowing for more nuanced predictions compared to univariate time series models.\n",
    "\n",
    "3. **Flexible Forecasting Horizon**: VAR model provides flexibility in forecasting horizons, enabling predictions for multiple time steps ahead, which is valuable for planning and decision-making in your multivariate dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Auto Regression RMSE: 52.22280401960267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "var_model = VAR(endog=X_train)\n",
    "\n",
    "var_model_fitted = var_model.fit()\n",
    "\n",
    "var_preds = var_model_fitted.forecast(y=X_train.values[-var_model_fitted.k_ar:], steps=len(X_test))\n",
    "\n",
    "var_sourcing_cost_preds = var_preds[:, 0]\n",
    "\n",
    "var_rmse = mean_squared_error(y_test, var_sourcing_cost_preds, squared=False)\n",
    "print(\"Vector Auto Regression RMSE:\", var_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model:\n",
    "\n",
    "1. **Simplicity and Transparency**: Linear Regression offers a simple and transparent modeling approach, making it easy to understand and interpret the relationship between the features and the target variable. This transparency is beneficial for explanatory analysis.\n",
    "\n",
    "2. **Efficiency with Large Datasets**: Linear Regression can handle large datasets efficiently, making it suitable for datasets with multiple features and a considerable number of data points like yours. It does so without significant computational overhead.\n",
    "\n",
    "3. **Baseline Performance**: Linear Regression provides a baseline performance metric against which more complex models can be compared. It serves as a starting point for model selection and evaluation in your multivariate dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 45.34866317998951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train, y_train)\n",
    "\n",
    "linear_reg_preds = linear_reg_model.predict(X_test)\n",
    "\n",
    "linear_reg_rmse = mean_squared_error(y_test, linear_reg_preds, squared=False)\n",
    "print(\"Linear Regression RMSE:\", linear_reg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate State-Space Model:\n",
    "\n",
    "1. **Dynamic and Flexible Modeling**: State-space models provide a dynamic and flexible framework for modeling time series data, allowing for the incorporation of exogenous variables like in your multivariate dataset. This capability enables capturing complex relationships between variables over time.\n",
    "\n",
    "2. **Separation of Components**: State-space models decompose the time series into different components (e.g., trend, seasonality, and noise), providing insights into the underlying structure of the data. By modeling each component separately, these models enable more accurate predictions.\n",
    "\n",
    "3. **Incorporation of Exogenous Variables**: State-space models can easily incorporate exogenous variables, allowing for the integration of external factors that may influence the time series behavior. This enhances the model's predictive capabilities for your multivariate dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\structural.py:508: SpecificationWarning: Specified model does not contain a stochastic element; irregular component added.\n",
      "  warn(\"Specified model does not contain a stochastic element;\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multivariate State-Space RMSE: 44.8530754718732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model_state_space = sm.tsa.UnobservedComponents(y_train, exog=X_train)\n",
    "results_state_space = model_state_space.fit()\n",
    "\n",
    "state_space_preds = results_state_space.forecast(steps=len(X_test), exog=X_test)\n",
    "\n",
    "state_space_rmse = mean_squared_error(y_test, state_space_preds, squared=False)\n",
    "print(\"Multivariate State-Space RMSE:\", state_space_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "After evaluating all the best-known Machine Learning Models and analyzing their results, I have concluded that the Multivariate State-Space Model and Multiple Linear Regression provide the best outputs compared to all the other models.\n",
    "\n",
    "### Performance Comparison:\n",
    "\n",
    "- **Random Forest RMSE:** 90.04\n",
    "- **XGBoost RMSE:** 84.35\n",
    "- **LightGBM RMSE:** 83.91\n",
    "- **Vector Auto Regression RMSE:** 52.22\n",
    "- **Linear Regression RMSE:** 45.35 (Execution Time: 0.0 sec)\n",
    "- **Multivariate State-Space RMSE:** 44.85 (Execution Time: 2 min 5 sec)\n",
    "\n",
    "I also observed that Linear Regression is much faster compared to the Multivariate State-Space model, with execution times of 0.0 seconds and 2 minutes 5 seconds, respectively.\n",
    "\n",
    "Based on the RMSE values, both Linear Regression and Multivariate State-Space models outperform other models, with Multivariate State-Space having a slightly lower RMSE. However, Linear Regression offers the advantage of faster execution.\n",
    "\n",
    "These findings suggest that for this particular dataset, both Linear Regression and Multivariate State-Space models are suitable choices, depending on the trade-off between speed and performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
